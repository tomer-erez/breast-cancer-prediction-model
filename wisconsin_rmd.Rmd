---
title: "Correlation between tumor size and it's classification"
author: "Tomer Erez and Tal Sadot"
date: 'May 2022'
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)
```

## Results {.tabset}

### Introduction


In this article, We analyze the (https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data) data set of breast cancer tumors.

In this data, we have cell measurements from 569 females whose diagnosis we know in advance.
The data was collected in order to develop methods to diagnose the cancer type, at a 95% confidence level. 


Our goal is to apply Machine learning methods and predict whether a given tumor is malignant or benign.
later on we will determine if there is a size difference between malignant and benign tumors.



Breast cancer is cancer that forms in the cells of the breasts.
Substantial support for breast cancer awareness and research funding has helped created advances in the diagnosis and treatment of breast cancer. Breast cancer survival rates have increased, and the number of deaths associated with this disease is steadily declining, largely due to factors such as earlier detection, a new personalized approach to treatment and a better understanding of the disease.
After skin cancer, breast cancer is the most common cancer diagnosed in women around the United States. Breast cancer can occur in both men and women, but it's far more common in women.

There is no single test that can accurately diagnose cancer. The complete evaluation of a patient usually requires a thorough history and physical examination along with diagnostic testing. Many tests are needed to determine whether a person has cancer, or if another condition (such as an infection) is mimicking the symptoms of cancer.

one way to diagnose cancer is done by Imaging tests. 
Imaging tests allow your doctor to examine your bones and internal organs in a noninvasive way. Imaging tests used in diagnosing cancer may include a computerized tomography (CT) scan, bone scan, magnetic resonance imaging (MRI), positron emission tomography (PET) scan, ultrasound and X-ray, among others.


  
We used the following libraries:



```{r packages,include=TRUE,warning=FALSE,message=FALSE}
library(tidyverse)
library(readr)
library(ggplot2)
library(cowplot)
library(hrbrthemes)
library(viridis)
library(oddsratio)
library(janitor)
library(corrplot)
library(caret)
library(InformationValue)
library(ISLR)
library(cvms)
library(ROCR)
library(tibble)
library(patchwork)
library(lattice)
```


### Loading data

```{r}

raw_data <-read.csv("data.csv", header=TRUE)
nrow(raw_data)
glimpse(raw_data)
#omit nukkriws
raw_data = raw_data[-33]
#find number of rows with missing data
sum(!complete.cases(raw_data))
sum(raw_data$diagnosis == "B", na.rm=TRUE)
#number of benign tumors
sum(raw_data$diagnosis == "M", na.rm=TRUE)
#number of malignant tumors
nrow(raw_data)
#total number of observations

```


Let's investigate as well the correlation between the features using `cor` function for `Pearson` correlation. as we are trying to develop a model that will rely on multiple uncorrelated parameters.


```{r fig.width=8, fig.height=8, correlation}
nc=ncol(raw_data)
df <- raw_data[,3:nc-1]
df$diagnosis <- as.integer(factor(df$diagnosis))-1
correlations <- cor(df,method="pearson")
corrplot(correlations, number.cex = .9, method = "square", 
         hclust.method = "ward", order = "FPC",
         type = "full", tl.cex=0.8,tl.col = "black")
```


The highest correlations are between:  

* `perimeter_mean` and `radius_worst`;  
* `area_worst` and `radius_worst`;  
* `perimeter_worst` and `radius_worst`, `perimeter_mean`, `area_worst`, `area_mean`, `radius_mean`;  
* `texture_mean` and `texture_worst`; 

'symmetry', 'texture', 'concavity_mean' and 'perimeter' have low correlation so we will explore those in our model












### Logistic regression

The reason we chose a logistic regression model rather than a linear one is that linear models provides correlation between quantifiable variables and a quantifiable result.
our model explores how  tumor's measurements effect the state of the tumor(malignant or benign).
further explanations on logistic transformation under the kogistic regression table


Overfitting a model is a condition where a statistical model begins to describe the random error in the data rather than the relationships between variables. This problem occurs when the model is too complex. In regression analysis, overfitting can produce misleading R-squared values, regression coefficients, and p-values. In this post, I explain how overfitting models is a problem and how you can identify and avoid it.

Overfit regression models have too many terms for the number of observations. When this occurs, the regression coefficients represent the noise rather than the genuine relationships in the population.

hence we will use 4 data parameters, which are relatively uncorrelated by analyzing the pearson correlation table.

the parameters are: perimeter_mean,symmetry_mean,texture_mean,concavity_mean.

Texture mean is the standard deviation of gray-scale values.
perimeter mean is mean size of the core tumor.
concavity mean is mean of severity of concave portions of the contour.
symmetry means symmetry grade of the tumor.





showing the data 
```{r show data}
wisconsin_LR<-raw_data%>%select(diagnosis,perimeter_mean,symmetry_mean,texture_mean,concavity_mean)
head(wisconsin_LR)
#swap malignant diagnosis with "1", benign with "0", encoding the target as factor
wisconsin_LR$diagnosis = factor(wisconsin_LR$diagnosis,levels = c('B', 'M'), labels = c(0, 1))
wisconsin_LR$symmetry_mean=wisconsin_LR$symmetry_mean*100
wisconsin_LR$concavity_mean=wisconsin_LR$concavity_mean*100
#normalize collum values
head(wisconsin_LR)
```


logistic regression and analyzing the findings
```{r}
wisconsin_LR=wisconsin_LR[order(wisconsin_LR$perimeter_mean,decreasing = FALSE),]
wisconsin_LR$diagnosis<-as.factor(wisconsin_LR$diagnosis)

mylogit <- glm(diagnosis ~perimeter_mean+symmetry_mean+texture_mean+concavity_mean ,data = wisconsin_LR, family = "binomial")

summary(mylogit)

```
odds ratio
```{r}

exp(coef(mylogit)[-1])

#we cab see texture and symmetry correlate stronger with malignancy form the odds ratio table.
# a rise in 1 unit tested (times 100 because we factored the column values) in symmetry mean means the likelihood of the tumor being malignant rises by a factor of 1.35

```


let's explain the coefficient table:

the equation that the model has built is of the form: p(y=malignant|X)=e to the exponent of the logistic model.

![logistic regression explained](C:\Users\97254\Documents\path for breast cancer data\log_equation.JPG)


test whether coef>0 on a confidence interval test


##R squared for logistic regression, also know as "McFadden R-squared"
```{r}
ll.null<-mylogit$null.deviance/-2
ll.proposed<-mylogit$deviance/-2
R_squared=(ll.null-ll.proposed)/ll.null
R_squared
# overall effect size, R squared, is 0.7531155, suggests that the model explains the residuals very well:
```


Assessing Model Fit:

In typical linear regression, we use R2 as a way to assess how well a model fits the data. This number ranges from 0 to 1, with higher values indicating better model fit.

However, there is no such R2 value for logistic regression. Instead, we can compute a metric known as McFadden’s R2, which ranges from 0 to just under 1. Values close to 0 indicate that the model has no predictive power. In practice, values over 0.40 indicate that a model fits the data very well.

We can compute McFadden’s R2 for our model using the pR2 function from the pscl package:

pscl::pR2(model)["McFadden"]

 McFadden 
0.7531155 
A value of 0.7531155 is quite high for McFadden’s R2, which indicates that our model fits the data very well and has high predictive power.




prediction model
```{r}
predicted.data<-data.frame(
  probability.of.Malignancy=mylogit$fitted.values,
  M_rate=wisconsin_LR$diagnosis)
predicted.data<-predicted.data[
  +order(predicted.data$probability.of.Malignancy,decreasing = FALSE),]
predicted.data$rank<-1:nrow(predicted.data)
ggplot(data=predicted.data,aes(x=wisconsin_LR$perimeter_mean,y=probability.of.Malignancy))+
  geom_point(aes(color=M_rate),alpha=1,shape=4,stroke=2)+
  xlab("Perimeter mean")+
  ylab("predicted probability of a tumor being malignant")


#our model seems to predict diagnosis at a solid leve.

# most malignant tumors are in the top right corner of the graph, which means our model estimates a high likelihood of those being malignant.
# and vice versa for benign tumors.

```


confusion matrix
```{r}

wisconsin_LR$d.m <- factor(wisconsin_LR$diagnosis)

inTrain <- createDataPartition(y = wisconsin_LR$d.m, p = 0.5, list = FALSE)
training <- wisconsin_LR[inTrain,]
testing <- wisconsin_LR[-inTrain,]


wis.prob = predict(mylogit, testing, type="response")
wis.pred = rep(0, dim(training)[1])
wis.pred[wis.prob > .5] = 1


pred_table<-table(wis.pred, training$d.m)
pred_table
mean(wis.pred == training$d.m)


ctable <- as.table(matrix(c(pred_table[1], pred_table[3], pred_table[2], pred_table[4]), nrow = 2, byrow = TRUE))
fourfoldplot(ctable, color = c("cyan", "pink"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")


#upward diagonal presents the number of incorrect predictions
#downward diagonal presents the number of correct predictions
#top row actual values
#top column predicted values
#indexing
#0,0 tp
#0,1 fp
#1,0 fn
#1,1 tn


# success rate of 0.8-0.85, depends on the random picking of data, representing an error rate of about 20-25%, and is therefore much more accurate than random guessing.


```



ROC:

```{r}

wis.pred=wis.pred[-285]
pred <- prediction(wis.pred, testing$diagnosis)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize=TRUE)
performance(pred, "tpr", "fpr")

```




### Hypothsis test






H0: mean of perimeter within malignant tumors equals perimeter within benign.

H1: mean of perimeter within malignant tumors is greater than perimeter within benign.





```{r}


wisconsin_HT<-raw_data%>%select(diagnosis,perimeter_mean)

wisconsin_HT$diagnosis <- as.factor(wisconsin_HT$diagnosis)

head(wisconsin_HT)
```


```{r,include=FALSE}


p1 <- ggplot(wisconsin_LR, aes(x=perimeter_mean, fill=diagnosis)) +
geom_density(alpha=0.4) +
theme(legend.position = "none")

p2 <- ggplot(wisconsin_LR, aes(x=symmetry_mean, fill=diagnosis)) +
geom_density(alpha=0.4) +
theme(legend.position = "none")
p3 <- ggplot(wisconsin_LR, aes(x=texture_mean, fill=diagnosis)) +
geom_density(alpha=0.4) +
theme(legend.position = "none")
p4 <- ggplot(wisconsin_LR, aes(x=concavity_mean, fill=diagnosis)) +
geom_density(alpha=0.4) +
theme(legend.position = "none")


```


showing the data distribution:


```{r data distribution}
(p1|p2)/(p3|p4)


```















analyzing symmetry differences between malignant and benign tumors
```{r}
b_s=wisconsin_LR%>%filter(diagnosis==0)%>%select(symmetry_mean)
m_s=wisconsin_LR%>%filter(diagnosis==1)%>%select(symmetry_mean)
m_s=unlist(m_s)
b_s=unlist(b_s)
shapiro.test(m_s)
shapiro.test(b_s)
# Neither symmetry means of benign nor malignant tumors distribute normally according to "Wilk Shapiro test".
```

Shapiro test to understand whether perimeter mean distributes normally in our data:
```{r}
b=wisconsin_HT%>%filter(diagnosis=="B")%>%select(perimeter_mean)
m=wisconsin_HT%>%filter(diagnosis=="M")%>%select(perimeter_mean)
m=unlist(m)
b=unlist(b)
shapiro.test(m)
shapiro.test(b)
#benign tumors clearly are normally distributed as p-value is bigger than alpha
#unlike malignant tumors, whose perimeter does not distribute normally
```




analyzing perimeter mean differences between malignant and benign tumors:
```{r}
ggplot(wisconsin_LR, aes(perimeter_mean)) + 
  geom_density(aes(data = perimeter_mean, fill = diagnosis), position = 'identity', alpha = 0.5) +
  labs(x = 'perimeter_mean', y = 'Density') + scale_fill_discrete(name = 'diagnosis') + scale_x_continuous(limits = c(0, 200))
```





analyzing perimeter differences between malignant and beingn tumors:
```{r}


t.test(formula=perimeter_mean~diagnosis,data=wisconsin_HT,alternative="less")




# malignant tumors are larger the benign ones.
```




```{r}
ggplot(data = raw_data, 
       aes(x = radius_mean, y = perimeter_mean, color = diagnosis)) +
  geom_point() +
  geom_hline(yintercept = 116.0, linetype = 'dashed', color = 'gray')+
  geom_vline(xintercept = 18.00, linetype = 'dashed', color = 'gray')+
  labs(title = 'Mean Perimeter and Mean Radius',
       subtitle = 'Malignant lumps can get relatively bigger than benigns',
       caption = 'Data owned by the University of Wisconsin',
       x = 'Mean Radius', y = 'Mean Perimeter') +
  annotate('text', x = 24, y = 150, 
           label = '45% of malignants are bigger than every observed benign',
           size = 2.3, angle = 45)
#Insights from graph
#Malignant lumps can get relatively bigger than benign lumps. This has the possibility of sparking up a hypothesis that malignant lumps begin as benigns. The #data as it stands has no time variable hence that would be difficult to establish with what we have. However, bigger lumps are more likely to be malignants.
```









### summary and conclusions

![All in good spirits!](C:\Users\97254\Documents\path for breast cancer data\meme.JPEG)



we learned that...








